import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

start_url = 'https://en.wikipedia.org/wiki/'
keywords = ['python', 'data science', 'machine learning']
max_depth = 2
visited_urls = set()

def crawl(url, depth=0):
    if depth > max_depth:
        return
    if url in visited_urls: 
        return
    try:
        response = requests.get(url)
        if response.status_code == 200: 
            visited_urls.add(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text().lower()
            for keyword in keywords:
                if keyword in text:
                    print(f"Keyword '{keyword}' found on: {url}")
            for link in soup.find_all('a', href=True):
                next_url = link['href']
                next_url = urljoin(url, next_url) 
                crawl(next_url, depth + 1)
    except Exception as e:
        print(f"Error crawling {url}: {str(e)}")

crawl(start_url)
