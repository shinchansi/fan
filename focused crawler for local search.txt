import requests
from bs4 import BeautifulSoup
import re
def crawl(seed_url, keyword, max_depth=3):
    visited_urls = set()

    def extract_information(url, soup):
        print(f"Extracting information from {url}")

    def is_valid_url(url):
        return re.search(keyword, url) is not None

    def recursive_crawl(url, depth=1):
        if depth > max_depth or url in visited_urls:
            return
        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                extract_information(url, soup)
                links = soup.find_all('a', href=True)
                for link in links:
                    next_url = link['href']
                    if is_valid_url(next_url):
                        recursive_crawl(next_url, depth + 1)
                visited_urls.add(url)
        except Exception as e:
            print(f"Error crawling {url}: {e}")

    recursive_crawl(seed_url)
if __name__ == "__main__":
    seed_url = "http://recurship.com/"
    keyword = "blog"
    max_depth = 3
    crawl(seed_url, keyword, max_depth)